{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import ants\n",
    "import argparse\n",
    "import shutil\n",
    "import logging\n",
    "import subprocess\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "import torchio as tio\n",
    "from glob2 import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpu.io\n",
    "schw_seed = mpu.io.read('utils/bgpd_seed.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"fold_0\": [\n",
      "        \"1028_18_4\",\n",
      "        \"1043_18_4\",\n",
      "        \"1072_19\",\n",
      "        \"1164_18\",\n",
      "        \"1170_18_4\",\n",
      "        \"1185_18_4\",\n",
      "        \"1302_18_4\",\n",
      "        \"1357_19_4\",\n",
      "        \"1362_18_4\",\n",
      "        \"1463_18_4\",\n",
      "        \"1501_18_4\",\n",
      "        \"1539_18\",\n",
      "        \"1546_18\",\n",
      "        \"1685_18_4\",\n",
      "        \"1770_18_4\",\n",
      "        \"349_18_4\",\n",
      "        \"573_18_4\",\n",
      "        \"668_18_4\",\n",
      "        \"705_18_4\",\n",
      "        \"746_19_4\",\n",
      "        \"826_18_4\",\n",
      "        \"856_19_4\",\n",
      "        \"923_18\",\n",
      "        \"979_18_4\",\n",
      "        \"990_18_4\",\n",
      "        \"Patient_1001316\",\n",
      "        \"Patient_107017\",\n",
      "        \"Patient_109017\",\n",
      "        \"Patient_111016\",\n",
      "        \"Patient_122315\",\n",
      "        \"Patient_12417\",\n",
      "        \"Patient_127916\",\n",
      "        \"Patient_131416\",\n",
      "        \"Patient_132216\",\n",
      "        \"Patient_133916\",\n",
      "        \"Patient_136715\",\n",
      "        \"Patient_136915\",\n",
      "        \"Patient_140316\",\n",
      "        \"Patient_146716\",\n",
      "        \"Patient_15817\",\n",
      "        \"Patient_1815\",\n",
      "        \"Patient_22117\",\n",
      "        \"Patient_24717\",\n",
      "        \"Patient_43316\",\n",
      "        \"Patient_43515\",\n",
      "        \"Patient_48417\",\n",
      "        \"Patient_48517\",\n",
      "        \"Patient_49617\",\n",
      "        \"Patient_52315\",\n",
      "        \"Patient_716\",\n",
      "        \"Patient_72715\",\n",
      "        \"Patient_8017\",\n",
      "        \"Patient_83217\",\n",
      "        \"Patient_84116\",\n",
      "        \"Patient_87114\",\n",
      "        \"Patient_88817\",\n",
      "        \"Patient_88917\",\n",
      "        \"Patient_90517\",\n",
      "        \"Patient_90616\",\n",
      "        \"Patient_99715\"\n",
      "    ],\n",
      "    \"fold_1\": [\n",
      "        \"1028_18_4\",\n",
      "        \"1043_18_4\",\n",
      "        \"1072_19\",\n",
      "        \"1164_18\",\n",
      "        \"1170_18_4\",\n",
      "        \"1185_18_4\",\n",
      "        \"1302_18_4\",\n",
      "        \"1357_19_4\",\n",
      "        \"1362_18_4\",\n",
      "        \"1463_18_4\",\n",
      "        \"1501_18_4\",\n",
      "        \"1539_18\",\n",
      "        \"1546_18\",\n",
      "        \"1685_18_4\",\n",
      "        \"1770_18_4\",\n",
      "        \"349_18_4\",\n",
      "        \"573_18_4\",\n",
      "        \"668_18_4\",\n",
      "        \"705_18_4\",\n",
      "        \"746_19_4\",\n",
      "        \"826_18_4\",\n",
      "        \"856_19_4\",\n",
      "        \"923_18\",\n",
      "        \"979_18_4\",\n",
      "        \"990_18_4\",\n",
      "        \"Patient_1001316\",\n",
      "        \"Patient_107017\",\n",
      "        \"Patient_109017\",\n",
      "        \"Patient_111016\",\n",
      "        \"Patient_122315\",\n",
      "        \"Patient_12417\",\n",
      "        \"Patient_127916\",\n",
      "        \"Patient_131416\",\n",
      "        \"Patient_132216\",\n",
      "        \"Patient_133916\",\n",
      "        \"Patient_136715\",\n",
      "        \"Patient_136915\",\n",
      "        \"Patient_140316\",\n",
      "        \"Patient_146716\",\n",
      "        \"Patient_15817\",\n",
      "        \"Patient_1815\",\n",
      "        \"Patient_22117\",\n",
      "        \"Patient_24717\",\n",
      "        \"Patient_43316\",\n",
      "        \"Patient_43515\",\n",
      "        \"Patient_48417\",\n",
      "        \"Patient_48517\",\n",
      "        \"Patient_49617\",\n",
      "        \"Patient_52315\",\n",
      "        \"Patient_716\",\n",
      "        \"Patient_72715\",\n",
      "        \"Patient_8017\",\n",
      "        \"Patient_83217\",\n",
      "        \"Patient_84116\",\n",
      "        \"Patient_87114\",\n",
      "        \"Patient_88817\",\n",
      "        \"Patient_88917\",\n",
      "        \"Patient_90517\",\n",
      "        \"Patient_90616\",\n",
      "        \"Patient_99715\"\n",
      "    ],\n",
      "    \"fold_2\": [\n",
      "        \"1028_18_4\",\n",
      "        \"1043_18_4\",\n",
      "        \"1072_19\",\n",
      "        \"1164_18\",\n",
      "        \"1170_18_4\",\n",
      "        \"1185_18_4\",\n",
      "        \"1302_18_4\",\n",
      "        \"1357_19_4\",\n",
      "        \"1362_18_4\",\n",
      "        \"1463_18_4\",\n",
      "        \"1501_18_4\",\n",
      "        \"1539_18\",\n",
      "        \"1546_18\",\n",
      "        \"1685_18_4\",\n",
      "        \"1770_18_4\",\n",
      "        \"349_18_4\",\n",
      "        \"573_18_4\",\n",
      "        \"668_18_4\",\n",
      "        \"705_18_4\",\n",
      "        \"746_19_4\",\n",
      "        \"826_18_4\",\n",
      "        \"856_19_4\",\n",
      "        \"923_18\",\n",
      "        \"979_18_4\",\n",
      "        \"990_18_4\",\n",
      "        \"Patient_1001316\",\n",
      "        \"Patient_107017\",\n",
      "        \"Patient_109017\",\n",
      "        \"Patient_111016\",\n",
      "        \"Patient_122315\",\n",
      "        \"Patient_12417\",\n",
      "        \"Patient_127916\",\n",
      "        \"Patient_131416\",\n",
      "        \"Patient_132216\",\n",
      "        \"Patient_133916\",\n",
      "        \"Patient_136715\",\n",
      "        \"Patient_136915\",\n",
      "        \"Patient_140316\",\n",
      "        \"Patient_146716\",\n",
      "        \"Patient_15817\",\n",
      "        \"Patient_1815\",\n",
      "        \"Patient_22117\",\n",
      "        \"Patient_24717\",\n",
      "        \"Patient_43316\",\n",
      "        \"Patient_43515\",\n",
      "        \"Patient_48417\",\n",
      "        \"Patient_48517\",\n",
      "        \"Patient_49617\",\n",
      "        \"Patient_52315\",\n",
      "        \"Patient_716\",\n",
      "        \"Patient_72715\",\n",
      "        \"Patient_8017\",\n",
      "        \"Patient_83217\",\n",
      "        \"Patient_84116\",\n",
      "        \"Patient_87114\",\n",
      "        \"Patient_88817\",\n",
      "        \"Patient_88917\",\n",
      "        \"Patient_90517\",\n",
      "        \"Patient_90616\",\n",
      "        \"Patient_99715\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "data = {'fold_0': [i[:-2] for i in schw_seed['fold_0']],\n",
    "        'fold_1': [i[:-2] for i in schw_seed['fold_1']],\n",
    "        'fold_2': [i[:-2] for i in schw_seed['fold_2']],\n",
    "       }\n",
    "print(json.dumps(data, indent=4))\n",
    "\n",
    "\n",
    "# mpu.io.write('utils/bgpd_seed.json', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Make tree datasets for each fold\n",
    "#### 2. Train and transform images\n",
    "#### 3. Check distributions for tumor, brain w/o  tumor and whole image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir('/anvar/public_datasets/preproc_study/gbm/6_hist/6_histogram/6_hist_brain_masks/')\n",
    "save_dir = '/anvar/public_datasets/preproc_study/gbm/6_hist/6_histogram'\n",
    "base_dir = '/anvar/public_datasets/preproc_study/gbm/3a_atlas/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ANTsImage (RPI)\n",
       "\t Pixel Type : float (float32)\n",
       "\t Components : 1\n",
       "\t Dimensions : (240, 240, 155)\n",
       "\t Spacing    : (1.0, 1.0, 1.0)\n",
       "\t Origin     : (-119.0, 129.0, -68.0)\n",
       "\t Direction  : [ 1.  0.  0.  0. -1.  0.  0.  0.  1.]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ants.image_read('/anvar/public_datasets/preproc_study/gbm/5_ss_shared/' + 'TCGA-02-0086/' + 'T1.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving masks in right orientation\n",
    "# mask_dir = '/anvar/public_datasets/preproc_study/gbm/6_hist/6_histogram/6_hist_brain_masks/'\n",
    "# orig_mask_dir = '/anvar/public_datasets/preproc_study/gbm/5_ss_shared/'\n",
    "# for patient in os.listdir(base_dir):\n",
    "#     if os.path.isdir(base_dir + patient):\n",
    "#         mask = ants.image_read(orig_mask_dir + patient + '/CT1_mask.nii.gz')\n",
    "#         mask_rpi = ants.reorient_image2(mask, orientation='LPI')\n",
    "#         ants.image_write(mask_rpi, mask_dir + patient + '_mask.nii.gz', ri=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(mask_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "subjects_list = []\n",
    "t1_list = []\n",
    "t2_list = []\n",
    "ct1_list = []\n",
    "fl_list = []\n",
    "\n",
    "for patient in os.listdir(base_dir):\n",
    "    if os.path.isdir(base_dir + patient):\n",
    "        subject = tio.Subject(\n",
    "            t1 = tio.ScalarImage(base_dir + patient + '/T1.nii.gz'),\n",
    "            t2 = tio.ScalarImage(base_dir + patient + '/T2.nii.gz'),\n",
    "            ct1 = tio.ScalarImage(base_dir + patient + '/CT1.nii.gz'),\n",
    "            fl = tio.ScalarImage(base_dir + patient + '/FLAIR.nii.gz'),\n",
    "            label = tio.LabelMap(base_dir + patient + '/CT1_SEG.nii.gz'))\n",
    "\n",
    "        subjects_list.append(subject)\n",
    "        t1_list.append(base_dir + patient + '/T1.nii.gz')\n",
    "        t2_list.append(base_dir + patient + '/T2.nii.gz')\n",
    "        ct1_list.append(base_dir + patient + '/CT1.nii.gz')\n",
    "        fl_list.append(base_dir + patient + '/FLAIR.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpu.io\n",
    "gbm_seed = mpu.io.read('utils/gbm_seed.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gbm_seed['fold_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/120 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "For landmarks there are  120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:56<00:00,  2.13it/s]\n",
      "100%|██████████| 120/120 [00:56<00:00,  2.11it/s]\n",
      "100%|██████████| 120/120 [00:56<00:00,  2.12it/s]\n",
      "100%|██████████| 120/120 [00:59<00:00,  2.03it/s]\n",
      "  0%|          | 0/120 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "For landmarks there are  120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [01:45<00:00,  1.14it/s]\n",
      "100%|██████████| 120/120 [01:52<00:00,  1.07it/s]\n",
      "100%|██████████| 120/120 [01:26<00:00,  1.39it/s]\n",
      "100%|██████████| 120/120 [01:24<00:00,  1.41it/s]\n",
      "  0%|          | 0/120 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "For landmarks there are  120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [01:41<00:00,  1.18it/s]\n",
      "100%|██████████| 120/120 [01:38<00:00,  1.22it/s]\n",
      "100%|██████████| 120/120 [01:30<00:00,  1.33it/s]\n",
      "100%|██████████| 120/120 [01:33<00:00,  1.28it/s]\n"
     ]
    }
   ],
   "source": [
    "from torchio.transforms import HistogramStandardization\n",
    "import argparse\n",
    "import logging\n",
    "import mpu.io\n",
    "from os import path\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "# parser.add_argument('--path', type=str, default='/anvar/public_datasets/preproc_study/bgpd/3a_atlas/', \n",
    "#                     help='root dir for subject sequences data')\n",
    "# parser.add_argument('--fixedfilename', type=list, default=['FLAIR.nii.gz'], help='name of file to register')\n",
    "# parser.add_argument('--maskfilename', type=list, default=['mask_GTV_FLAIR.nii.gz'], help='name of mask to register to RPI')\n",
    "# parser.add_argument('--movingfilenames', type=list, default=['CT1.nii.gz','T2.nii.gz','T1.nii.gz'], help='names of files')\n",
    "# parser.add_argument('--output', type=str, default='/mnt/public_data/preproc_study/bgpd/6_hist/', \n",
    "#                     help= 'output folder')\n",
    "# parser.add_argument('--seed', type=str, default='utils/example.json', help= 'mode individual or shared ')\n",
    "# parser.add_argument('--device', type=str, default='cpu', help= 'gpu or cpu, if gpu - should be `int` ')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# base_dir = args.path\n",
    "# save_dir = args.autput\n",
    "# seed = mpu.io.read(args.seed)\n",
    "# mask_name = args.maskfilename\n",
    "\n",
    "base_dir = '/anvar/public_datasets/preproc_study/bgpd/3a_atlas/'\n",
    "save_dir = '/anvar/public_datasets/preproc_study/bgpd/6_hist/'\n",
    "seed = mpu.io.read('utils/bgpd_seed.json')\n",
    "mask_name = 'mask_GTV_FLAIR.nii.gz'\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "    \n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "# logging.basicConfig(filename=args.output + \"logging.txt\", level=logging.INFO, format='[%(asctime)s.%(msecs)03d] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "# Creating a dataset\n",
    "subjects_list = []\n",
    "for patient in os.listdir(base_dir):\n",
    "    if os.path.isdir(base_dir + patient):\n",
    "        subject = tio.Subject(\n",
    "            t1 = tio.ScalarImage(base_dir + patient + '/T1.nii.gz'),\n",
    "            t2 = tio.ScalarImage(base_dir + patient + '/T2.nii.gz'),\n",
    "            ct1 = tio.ScalarImage(base_dir + patient + '/CT1.nii.gz'),\n",
    "            fl = tio.ScalarImage(base_dir + patient + '/FLAIR.nii.gz'))\n",
    "\n",
    "        subjects_list.append(subject)\n",
    "\n",
    "# Separate dataset for each fold\n",
    "for fold in ['fold_0', 'fold_1', 'fold_2']:\n",
    "    print(len(seed[fold]))\n",
    "#     logging.info(fold, \" started.\")\n",
    "    # Create dataset\n",
    "    temp_t1_list = []\n",
    "    temp_t2_list = []\n",
    "    temp_ct1_list = []\n",
    "    temp_fl_list = []\n",
    "\n",
    "    for patient in os.listdir(base_dir):\n",
    "        if patient not in seed[fold]:\n",
    "            if os.path.isdir(base_dir + patient):\n",
    "                \n",
    "                subjects_list.append(subject)\n",
    "                temp_t1_list.append(base_dir + patient + '/T1.nii.gz')\n",
    "                temp_t2_list.append(base_dir + patient + '/T2.nii.gz')\n",
    "                temp_ct1_list.append(base_dir + patient + '/CT1.nii.gz')\n",
    "                temp_fl_list.append(base_dir + patient + '/FLAIR.nii.gz')\n",
    "\n",
    "    print('For landmarks there are ', len(temp_t1_list))\n",
    "    # logging.info(\"Training T1 landmarks started.\")\n",
    "    t1_landmarks = HistogramStandardization.train(temp_t1_list)\n",
    "    # logging.info(\"Training T2 landmarks started.\")\n",
    "    t2_landmarks = HistogramStandardization.train(temp_t2_list)\n",
    "    # logging.info(\"Training CT1 landmarks started.\")\n",
    "    ct1_landmarks = HistogramStandardization.train(temp_ct1_list)\n",
    "    # logging.info(\"Training FLAIR landmarks started.\")\n",
    "    fl_landmarks = HistogramStandardization.train(temp_fl_list)\n",
    "\n",
    "    # Saving landmarks\n",
    "    landmarks_dict = {\n",
    "    't1': t1_landmarks,\n",
    "    't2': t2_landmarks,\n",
    "    'ct1': ct1_landmarks,\n",
    "    'fl': fl_landmarks\n",
    "    }\n",
    "\n",
    "    hist_standardize = tio.HistogramStandardization(landmarks_dict)\n",
    "\n",
    "        # Apply transforms\n",
    "    for i in range(0, len(os.listdir(base_dir))):\n",
    "        # Check if it is logging file instead of a folder\n",
    "        if os.path.isdir(base_dir + os.listdir(base_dir)[i]):\n",
    "            os.makedirs(save_dir + '/6_hist_{}/'.format(fold) + os.listdir(base_dir)[i], exist_ok= True)\n",
    "\n",
    "            if len(os.listdir(save_dir + '/6_hist_{}/'.format(fold) + os.listdir(base_dir)[i])) < 4:\n",
    "                # logging.info(\"Saving patient\", os.listdir(base_dir)[i], fold)\n",
    "                hist_standard = hist_standardize(subjects_list[i])\n",
    "                hist_standard['t1'].save(save_dir + '/6_hist_{}/'.format(fold) + os.listdir(base_dir)[i] +'/T1.nii.gz')\n",
    "                hist_standard['ct1'].save(save_dir + '/6_hist_{}/'.format(fold) + os.listdir(base_dir)[i] +'/CT1.nii.gz')\n",
    "                hist_standard['fl'].save(save_dir + '/6_hist_{}/'.format(fold) + os.listdir(base_dir)[i] +'/FLAIR.nii.gz')\n",
    "                hist_standard['t2'].save(save_dir + '/6_hist_{}/'.format(fold) + os.listdir(base_dir)[i] +'/T2.nii.gz')\n",
    "                shutil.copy(base_dir + os.listdir(base_dir)[i] + '/' + mask_name,\n",
    "                save_dir + '/6_hist_{}/'.format(fold) + os.listdir(base_dir)[i] +  '/' + mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/68 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "For landmarks there are  68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:31<00:00,  2.13it/s]\n",
      "100%|██████████| 68/68 [00:32<00:00,  2.12it/s]\n",
      "100%|██████████| 68/68 [00:33<00:00,  2.05it/s]\n",
      "100%|██████████| 68/68 [00:33<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCGA-06-0142\n",
      "TCGA-02-0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/68 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "For landmarks there are  68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:31<00:00,  2.14it/s]\n",
      "100%|██████████| 68/68 [00:34<00:00,  1.99it/s]\n",
      "100%|██████████| 68/68 [00:36<00:00,  1.85it/s]\n",
      "100%|██████████| 68/68 [00:35<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCGA-06-0142\n",
      "TCGA-02-0037\n",
      "TCGA-08-0520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/68 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "For landmarks there are  68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:31<00:00,  2.15it/s]\n",
      "100%|██████████| 68/68 [00:31<00:00,  2.13it/s]\n",
      "100%|██████████| 68/68 [00:32<00:00,  2.07it/s]\n",
      "100%|██████████| 68/68 [00:33<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCGA-06-0142\n",
      "TCGA-02-0037\n",
      "TCGA-08-0520\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "# parser.add_argument('--path', type=str, default='/anvar/public_datasets/preproc_study/bgpd/3a_atlas/', \n",
    "#                     help='root dir for subject sequences data')\n",
    "# parser.add_argument('--fixedfilename', type=list, default=['FLAIR.nii.gz'], help='name of file to register')\n",
    "# parser.add_argument('--maskfilename', type=list, default=['mask_GTV_FLAIR.nii.gz'], help='name of mask to register to RPI')\n",
    "# parser.add_argument('--movingfilenames', type=list, default=['CT1.nii.gz','T2.nii.gz','T1.nii.gz'], help='names of files')\n",
    "# parser.add_argument('--output', type=str, default='/mnt/public_data/preproc_study/bgpd/6_hist/', \n",
    "#                     help= 'output folder')\n",
    "# parser.add_argument('--seed', type=str, default='utils/example.json', help= 'mode individual or shared ')\n",
    "# parser.add_argument('--device', type=str, default='cpu', help= 'gpu or cpu, if gpu - should be `int` ')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# base_dir = args.path\n",
    "# save_dir = args.autput\n",
    "# seed = mpu.io.read(args.seed)\n",
    "# mask_name = args.maskfilename\n",
    "\n",
    "base_dir = '/anvar/public_datasets/preproc_study/gbm/3a_atlas/'\n",
    "save_dir = '/anvar/public_datasets/preproc_study/gbm/6_hist/6_histogram/'\n",
    "seed = mpu.io.read('utils/gbm_seed.json')\n",
    "mask_name = 'CT1_SEG.nii.gz'\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "    \n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "# logging.basicConfig(filename=args.output + \"logging.txt\", level=logging.INFO, format='[%(asctime)s.%(msecs)03d] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "# Creating a dataset\n",
    "subjects_list = []\n",
    "for patient in os.listdir(base_dir):\n",
    "    if os.path.isdir(base_dir + patient):\n",
    "        subject = tio.Subject(\n",
    "            t1 = tio.ScalarImage(base_dir + patient + '/T1.nii.gz'),\n",
    "            t2 = tio.ScalarImage(base_dir + patient + '/T2.nii.gz'),\n",
    "            ct1 = tio.ScalarImage(base_dir + patient + '/CT1.nii.gz'),\n",
    "            fl = tio.ScalarImage(base_dir + patient + '/FLAIR.nii.gz'))\n",
    "\n",
    "        subjects_list.append(subject)\n",
    "\n",
    "# Separate dataset for each fold\n",
    "for fold in ['fold_0', 'fold_1', 'fold_2']:\n",
    "    print(len(seed[fold]))\n",
    "#     logging.info(fold, \" started.\")\n",
    "    # Create dataset\n",
    "    temp_t1_list = []\n",
    "    temp_t2_list = []\n",
    "    temp_ct1_list = []\n",
    "    temp_fl_list = []\n",
    "\n",
    "    for patient in os.listdir(base_dir):\n",
    "        if patient not in seed[fold]:\n",
    "            if os.path.isdir(base_dir + patient):\n",
    "                \n",
    "                subjects_list.append(subject)\n",
    "                temp_t1_list.append(base_dir + patient + '/T1.nii.gz')\n",
    "                temp_t2_list.append(base_dir + patient + '/T2.nii.gz')\n",
    "                temp_ct1_list.append(base_dir + patient + '/CT1.nii.gz')\n",
    "                temp_fl_list.append(base_dir + patient + '/FLAIR.nii.gz')\n",
    "\n",
    "    print('For landmarks there are ', len(temp_t1_list))\n",
    "    # logging.info(\"Training T1 landmarks started.\")\n",
    "    t1_landmarks = HistogramStandardization.train(temp_t1_list)\n",
    "    # logging.info(\"Training T2 landmarks started.\")\n",
    "    t2_landmarks = HistogramStandardization.train(temp_t2_list)\n",
    "    # logging.info(\"Training CT1 landmarks started.\")\n",
    "    ct1_landmarks = HistogramStandardization.train(temp_ct1_list)\n",
    "    # logging.info(\"Training FLAIR landmarks started.\")\n",
    "    fl_landmarks = HistogramStandardization.train(temp_fl_list)\n",
    "\n",
    "    # Saving landmarks\n",
    "    landmarks_dict = {\n",
    "    't1': t1_landmarks,\n",
    "    't2': t2_landmarks,\n",
    "    'ct1': ct1_landmarks,\n",
    "    'fl': fl_landmarks\n",
    "    }\n",
    "\n",
    "    hist_standardize = tio.HistogramStandardization(landmarks_dict)\n",
    "\n",
    "        # Apply transforms\n",
    "    for i in range(0, len(os.listdir(base_dir))):\n",
    "        # Check if it is logging file instead of a folder\n",
    "        if os.path.isdir(base_dir + os.listdir(base_dir)[i]):\n",
    "            os.makedirs(save_dir + '/6_hist_{}/'.format(fold) + os.listdir(base_dir)[i], exist_ok= True)\n",
    "\n",
    "            if len(os.listdir(save_dir + '/6_hist_{}/'.format(fold) + os.listdir(base_dir)[i])) < 5:\n",
    "                print(os.listdir(base_dir)[i])\n",
    "                # logging.info(\"Saving patient\", os.listdir(base_dir)[i], fold)\n",
    "                hist_standard = hist_standardize(subjects_list[i])\n",
    "                hist_standard['t1'].save(save_dir + '/6_hist_{}/'.format(fold) + os.listdir(base_dir)[i] +'/T1.nii.gz')\n",
    "                hist_standard['ct1'].save(save_dir + '/6_hist_{}/'.format(fold) + os.listdir(base_dir)[i] +'/CT1.nii.gz')\n",
    "                hist_standard['fl'].save(save_dir + '/6_hist_{}/'.format(fold) + os.listdir(base_dir)[i] +'/FLAIR.nii.gz')\n",
    "                hist_standard['t2'].save(save_dir + '/6_hist_{}/'.format(fold) + os.listdir(base_dir)[i] +'/T2.nii.gz')\n",
    "                shutil.copy(base_dir + os.listdir(base_dir)[i] + '/' + mask_name,\n",
    "                save_dir + '/6_hist_{}/'.format(fold) + os.listdir(base_dir)[i] +  '/' + mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_0 102\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "logging.txt\n",
      "True\n",
      "True\n",
      "True\n",
      "fold_1 102\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "logging.txt\n",
      "True\n",
      "True\n",
      "True\n",
      "fold_2 102\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "logging.txt\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# cheching packages\n",
    "for fold in ['fold_0', 'fold_1', 'fold_2']:\n",
    "    print( fold, len(os.listdir(save_dir + '/6_hist_{}/'.format(fold))))\n",
    "    for patient in os.listdir(base_dir):\n",
    "        try:\n",
    "            print(len(os.listdir(save_dir + '/6_hist_{}/'.format(fold) + patient )) == 5)\n",
    "        except:\n",
    "            print(patient)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6_hist_fold_1', '6_hist_fold_2', '6_hist_fold_0']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_tumor_dataframe = pd.DataFrame(columns = bins)\n",
    "hist_brain_dataframe = pd.DataFrame(columns = bins)\n",
    "orig_tumor_dataframe = pd.DataFrame(columns = bins)\n",
    "orig_brain_dataframe = pd.DataFrame(columns = bins)\n",
    "\n",
    "for fold in ['fold_0']:\n",
    "    for patient in os.listdir(base_dir):\n",
    "        if os.path.isdir(base_dir + patient):\n",
    "#             print(patient)\n",
    "            hist_img_path = glob(save_dir + '/6_hist_{}/'.format(fold) + patient +'/CT1.nii.gz')[0]\n",
    "            orig_img_path = glob(base_dir + patient + '/CT1.nii.gz')[0]\n",
    "            tumor_mask_path = glob(base_dir + patient + '/CT1_SEG.nii.gz')[0]\n",
    "            brain_mask_path = glob(save_dir + '/6_hist_brain_masks/' +'{}_mask.nii.gz'.format(patient))[0]   \n",
    "\n",
    "            hist_img = ants.image_read(hist_img_path)\n",
    "            orig_img = ants.image_read(orig_img_path)\n",
    "            tumor_mask = ants.image_read(tumor_mask_path)\n",
    "            brain_mask = ants.image_read(brain_mask_path)\n",
    "\n",
    "            tumor_mask = tumor_mask.numpy() > 0\n",
    "            brain_mask = brain_mask.numpy() > 0\n",
    "            hist_tumor= hist_img.numpy()[tumor_mask]\n",
    "            orig_tumor = orig_img.numpy()[tumor_mask]\n",
    "\n",
    "            hist_brain = hist_img.numpy()[(~tumor_mask)&brain_mask]                 \n",
    "            orig_brain = orig_img.numpy()[(~tumor_mask)&brain_mask]  \n",
    "\n",
    "            a_series = pd.Series(np.histogram(hist_tumor, bins = 100, normed =True)[0], bins )\n",
    "            hist_tumor_dataframe = hist_tumor_dataframe.append(a_series, ignore_index=True)     \n",
    "\n",
    "            b_series = pd.Series(np.histogram(hist_brain, bins = 100, normed =True)[0], bins )\n",
    "            hist_brain_dataframe = hist_brain_dataframe.append(b_series, ignore_index=True)    \n",
    "\n",
    "            c_series = pd.Series(np.histogram(orig_brain, bins = 100, normed =True)[0], bins )\n",
    "            orig_brain_dataframe = orig_brain_dataframe.append(c_series, ignore_index=True)    \n",
    "\n",
    "            d_series = pd.Series(np.histogram(orig_tumor, bins = 100, normed =True)[0], bins )\n",
    "            orig_tumor_dataframe = orig_tumor_dataframe.append(d_series, ignore_index=True)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(0,1, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist_brain_dataframe.columns, hist_brain_dataframe.std(), label = 'Hist brain')\n",
    "plt.plot(hist_brain_dataframe.columns, orig_brain_dataframe.std(), label = 'Original brain')\n",
    "plt.plot(hist_brain_dataframe.columns, orig_tumor_dataframe.std(), label = 'Original tumor')\n",
    "plt.plot(hist_brain_dataframe.columns, hist_tumor_dataframe.std(), label = 'Hist tumor')\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
