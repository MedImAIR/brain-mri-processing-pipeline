{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "972c369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1a05b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold-0\tfold-1\tfold-2\n"
     ]
    }
   ],
   "source": [
    "!cd /results/gbm_results/gbm_2a_interp && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee02ac91",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 31.75 GiB total capacity; 892.55 MiB already allocated; 22.50 MiB free; 916.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24865/2218152168.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/results/gbm_results/gbm_4a_resamp/fold-2/checkpoints/best_epoch=198-dice_mean=75.08.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    669\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1003\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    973\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m         loaded_storages[key] = torch.storage.TypedStorage(\n\u001b[0;32m--> 960\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m             dtype=dtype)\n\u001b[1;32m    962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mstorage_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[0;31m# We may need to call lazy init again if we are a forked child\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;31m# del _CudaBase.__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 31.75 GiB total capacity; 892.55 MiB already allocated; 22.50 MiB free; 916.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "weights = torch.load('/results/gbm_results/gbm_4a_resamp/fold-2/checkpoints/best_epoch=198-dice_mean=75.08.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1fb02ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layers = ['model.output_block.conv.conv.weight','model.output_block.conv.conv.bias',\n",
    "              'model.deep_supervision_heads.0.conv.conv.weight','model.deep_supervision_heads.0.conv.conv.bias', \n",
    "               'model.deep_supervision_heads.1.conv.conv.weight','model.deep_supervision_heads.1.conv.conv.bias', \n",
    "               'model.deep_supervision_heads.2.conv.conv.weight','model.deep_supervision_heads.2.conv.conv.bias',\n",
    "               'model.deep_supervision_heads.3.conv.conv.weight','model.deep_supervision_heads.3.conv.conv.bias',\n",
    "               'model.deep_supervision_heads.4.conv.conv.weight','model.deep_supervision_heads.4.conv.conv.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00cb1c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_layers = ['model.output_block.conv.conv.weight', 'model.output_block.conv.conv.bias', 'model.deep_supervision_heads.0.conv.conv.weight','model.deep_supervision_heads.0.conv.conv.bias', \n",
    "               'model.deep_supervision_heads.1.conv.conv.weight','model.deep_supervision_heads.1.conv.conv.bias', \n",
    "               'model.deep_supervision_heads.2.conv.conv.weight','model.deep_supervision_heads.2.conv.conv.bias',\n",
    "               'model.deep_supervision_heads.3.conv.conv.weight','model.deep_supervision_heads.3.conv.conv.bias',\n",
    "               'model.deep_supervision_heads.4.conv.conv.weight','model.deep_supervision_heads.4.conv.conv.bias',\n",
    "              'model.skip_layers.next_layer.next_layer.next_layer.next_layer.next_layer.super_head.conv.conv.weight',\n",
    "              'model.skip_layers.next_layer.next_layer.next_layer.next_layer.next_layer.super_head.conv.conv.bias',\n",
    "              'model.skip_layers.next_layer.next_layer.next_layer.next_layer.super_head.conv.conv.weight',\n",
    "              'model.skip_layers.next_layer.next_layer.next_layer.next_layer.super_head.conv.conv.bias',\n",
    "              'model.skip_layers.next_layer.next_layer.next_layer.super_head.conv.conv.weight',\n",
    "              'model.skip_layers.next_layer.next_layer.next_layer.super_head.conv.conv.bias',\n",
    "              'model.skip_layers.next_layer.next_layer.super_head.conv.conv.weight',\n",
    "              'model.skip_layers.next_layer.next_layer.super_head.conv.conv.bias',\n",
    "              'model.skip_layers.next_layer.super_head.conv.conv.weight', 'model.skip_layers.next_layer.super_head.conv.conv.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d00b676d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.input_block.conv1.conv.weight\n",
      "torch.Size([64, 5, 3, 3, 3])\n",
      "model.input_block.conv2.conv.weight\n",
      "torch.Size([64, 64, 3, 3, 3])\n",
      "model.input_block.norm1.weight\n",
      "torch.Size([64])\n",
      "model.input_block.norm1.bias\n",
      "torch.Size([64])\n",
      "model.input_block.norm2.weight\n",
      "torch.Size([64])\n",
      "model.input_block.norm2.bias\n",
      "torch.Size([64])\n",
      "model.downsamples.0.conv1.conv.weight\n",
      "torch.Size([96, 64, 3, 3, 3])\n",
      "model.downsamples.0.conv2.conv.weight\n",
      "torch.Size([96, 96, 3, 3, 3])\n",
      "model.downsamples.0.norm1.weight\n",
      "torch.Size([96])\n",
      "model.downsamples.0.norm1.bias\n",
      "torch.Size([96])\n",
      "model.downsamples.0.norm2.weight\n",
      "torch.Size([96])\n",
      "model.downsamples.0.norm2.bias\n",
      "torch.Size([96])\n",
      "model.downsamples.1.conv1.conv.weight\n",
      "torch.Size([128, 96, 3, 3, 3])\n",
      "model.downsamples.1.conv2.conv.weight\n",
      "torch.Size([128, 128, 3, 3, 3])\n",
      "model.downsamples.1.norm1.weight\n",
      "torch.Size([128])\n",
      "model.downsamples.1.norm1.bias\n",
      "torch.Size([128])\n",
      "model.downsamples.1.norm2.weight\n",
      "torch.Size([128])\n",
      "model.downsamples.1.norm2.bias\n",
      "torch.Size([128])\n",
      "model.downsamples.2.conv1.conv.weight\n",
      "torch.Size([192, 128, 3, 3, 3])\n",
      "model.downsamples.2.conv2.conv.weight\n",
      "torch.Size([192, 192, 3, 3, 3])\n",
      "model.downsamples.2.norm1.weight\n",
      "torch.Size([192])\n",
      "model.downsamples.2.norm1.bias\n",
      "torch.Size([192])\n",
      "model.downsamples.2.norm2.weight\n",
      "torch.Size([192])\n",
      "model.downsamples.2.norm2.bias\n",
      "torch.Size([192])\n",
      "model.downsamples.3.conv1.conv.weight\n",
      "torch.Size([256, 192, 3, 3, 3])\n",
      "model.downsamples.3.conv2.conv.weight\n",
      "torch.Size([256, 256, 3, 3, 3])\n",
      "model.downsamples.3.norm1.weight\n",
      "torch.Size([256])\n",
      "model.downsamples.3.norm1.bias\n",
      "torch.Size([256])\n",
      "model.downsamples.3.norm2.weight\n",
      "torch.Size([256])\n",
      "model.downsamples.3.norm2.bias\n",
      "torch.Size([256])\n",
      "model.downsamples.4.conv1.conv.weight\n",
      "torch.Size([384, 256, 3, 3, 3])\n",
      "model.downsamples.4.conv2.conv.weight\n",
      "torch.Size([384, 384, 3, 3, 3])\n",
      "model.downsamples.4.norm1.weight\n",
      "torch.Size([384])\n",
      "model.downsamples.4.norm1.bias\n",
      "torch.Size([384])\n",
      "model.downsamples.4.norm2.weight\n",
      "torch.Size([384])\n",
      "model.downsamples.4.norm2.bias\n",
      "torch.Size([384])\n",
      "model.bottleneck.conv1.conv.weight\n",
      "torch.Size([512, 384, 3, 3, 3])\n",
      "model.bottleneck.conv2.conv.weight\n",
      "torch.Size([512, 512, 3, 3, 3])\n",
      "model.bottleneck.norm1.weight\n",
      "torch.Size([512])\n",
      "model.bottleneck.norm1.bias\n",
      "torch.Size([512])\n",
      "model.bottleneck.norm2.weight\n",
      "torch.Size([512])\n",
      "model.bottleneck.norm2.bias\n",
      "torch.Size([512])\n",
      "model.upsamples.0.transp_conv.conv.weight\n",
      "torch.Size([512, 384, 2, 2, 2])\n",
      "model.upsamples.0.transp_conv.conv.bias\n",
      "torch.Size([384])\n",
      "model.upsamples.0.conv_block.conv1.conv.weight\n",
      "torch.Size([384, 768, 3, 3, 3])\n",
      "model.upsamples.0.conv_block.conv2.conv.weight\n",
      "torch.Size([384, 384, 3, 3, 3])\n",
      "model.upsamples.0.conv_block.norm1.weight\n",
      "torch.Size([384])\n",
      "model.upsamples.0.conv_block.norm1.bias\n",
      "torch.Size([384])\n",
      "model.upsamples.0.conv_block.norm2.weight\n",
      "torch.Size([384])\n",
      "model.upsamples.0.conv_block.norm2.bias\n",
      "torch.Size([384])\n",
      "model.upsamples.1.transp_conv.conv.weight\n",
      "torch.Size([384, 256, 2, 2, 2])\n",
      "model.upsamples.1.transp_conv.conv.bias\n",
      "torch.Size([256])\n",
      "model.upsamples.1.conv_block.conv1.conv.weight\n",
      "torch.Size([256, 512, 3, 3, 3])\n",
      "model.upsamples.1.conv_block.conv2.conv.weight\n",
      "torch.Size([256, 256, 3, 3, 3])\n",
      "model.upsamples.1.conv_block.norm1.weight\n",
      "torch.Size([256])\n",
      "model.upsamples.1.conv_block.norm1.bias\n",
      "torch.Size([256])\n",
      "model.upsamples.1.conv_block.norm2.weight\n",
      "torch.Size([256])\n",
      "model.upsamples.1.conv_block.norm2.bias\n",
      "torch.Size([256])\n",
      "model.upsamples.2.transp_conv.conv.weight\n",
      "torch.Size([256, 192, 2, 2, 2])\n",
      "model.upsamples.2.transp_conv.conv.bias\n",
      "torch.Size([192])\n",
      "model.upsamples.2.conv_block.conv1.conv.weight\n",
      "torch.Size([192, 384, 3, 3, 3])\n",
      "model.upsamples.2.conv_block.conv2.conv.weight\n",
      "torch.Size([192, 192, 3, 3, 3])\n",
      "model.upsamples.2.conv_block.norm1.weight\n",
      "torch.Size([192])\n",
      "model.upsamples.2.conv_block.norm1.bias\n",
      "torch.Size([192])\n",
      "model.upsamples.2.conv_block.norm2.weight\n",
      "torch.Size([192])\n",
      "model.upsamples.2.conv_block.norm2.bias\n",
      "torch.Size([192])\n",
      "model.upsamples.3.transp_conv.conv.weight\n",
      "torch.Size([192, 128, 2, 2, 2])\n",
      "model.upsamples.3.transp_conv.conv.bias\n",
      "torch.Size([128])\n",
      "model.upsamples.3.conv_block.conv1.conv.weight\n",
      "torch.Size([128, 256, 3, 3, 3])\n",
      "model.upsamples.3.conv_block.conv2.conv.weight\n",
      "torch.Size([128, 128, 3, 3, 3])\n",
      "model.upsamples.3.conv_block.norm1.weight\n",
      "torch.Size([128])\n",
      "model.upsamples.3.conv_block.norm1.bias\n",
      "torch.Size([128])\n",
      "model.upsamples.3.conv_block.norm2.weight\n",
      "torch.Size([128])\n",
      "model.upsamples.3.conv_block.norm2.bias\n",
      "torch.Size([128])\n",
      "model.upsamples.4.transp_conv.conv.weight\n",
      "torch.Size([128, 96, 2, 2, 2])\n",
      "model.upsamples.4.transp_conv.conv.bias\n",
      "torch.Size([96])\n",
      "model.upsamples.4.conv_block.conv1.conv.weight\n",
      "torch.Size([96, 192, 3, 3, 3])\n",
      "model.upsamples.4.conv_block.conv2.conv.weight\n",
      "torch.Size([96, 96, 3, 3, 3])\n",
      "model.upsamples.4.conv_block.norm1.weight\n",
      "torch.Size([96])\n",
      "model.upsamples.4.conv_block.norm1.bias\n",
      "torch.Size([96])\n",
      "model.upsamples.4.conv_block.norm2.weight\n",
      "torch.Size([96])\n",
      "model.upsamples.4.conv_block.norm2.bias\n",
      "torch.Size([96])\n",
      "model.upsamples.5.transp_conv.conv.weight\n",
      "torch.Size([96, 64, 2, 2, 2])\n",
      "model.upsamples.5.transp_conv.conv.bias\n",
      "torch.Size([64])\n",
      "model.upsamples.5.conv_block.conv1.conv.weight\n",
      "torch.Size([64, 128, 3, 3, 3])\n",
      "model.upsamples.5.conv_block.conv2.conv.weight\n",
      "torch.Size([64, 64, 3, 3, 3])\n",
      "model.upsamples.5.conv_block.norm1.weight\n",
      "torch.Size([64])\n",
      "model.upsamples.5.conv_block.norm1.bias\n",
      "torch.Size([64])\n",
      "model.upsamples.5.conv_block.norm2.weight\n",
      "torch.Size([64])\n",
      "model.upsamples.5.conv_block.norm2.bias\n",
      "torch.Size([64])\n",
      "model.output_block.conv.conv.weight\n",
      "torch.Size([3, 64, 1, 1, 1])\n",
      "model.output_block.conv.conv.bias\n",
      "torch.Size([3])\n",
      "model.deep_supervision_heads.0.conv.conv.weight\n",
      "torch.Size([3, 96, 1, 1, 1])\n",
      "model.deep_supervision_heads.0.conv.conv.bias\n",
      "torch.Size([3])\n",
      "model.deep_supervision_heads.1.conv.conv.weight\n",
      "torch.Size([3, 128, 1, 1, 1])\n",
      "model.deep_supervision_heads.1.conv.conv.bias\n",
      "torch.Size([3])\n",
      "model.deep_supervision_heads.2.conv.conv.weight\n",
      "torch.Size([3, 192, 1, 1, 1])\n",
      "model.deep_supervision_heads.2.conv.conv.bias\n",
      "torch.Size([3])\n",
      "model.deep_supervision_heads.3.conv.conv.weight\n",
      "torch.Size([3, 256, 1, 1, 1])\n",
      "model.deep_supervision_heads.3.conv.conv.bias\n",
      "torch.Size([3])\n",
      "model.deep_supervision_heads.4.conv.conv.weight\n",
      "torch.Size([3, 384, 1, 1, 1])\n",
      "model.deep_supervision_heads.4.conv.conv.bias\n",
      "torch.Size([3])\n",
      "model.skip_layers.downsample.conv1.conv.weight\n",
      "torch.Size([64, 5, 3, 3, 3])\n",
      "model.skip_layers.downsample.conv2.conv.weight\n",
      "torch.Size([64, 64, 3, 3, 3])\n",
      "model.skip_layers.downsample.norm1.weight\n",
      "torch.Size([64])\n",
      "model.skip_layers.downsample.norm1.bias\n",
      "torch.Size([64])\n",
      "model.skip_layers.downsample.norm2.weight\n",
      "torch.Size([64])\n",
      "model.skip_layers.downsample.norm2.bias\n",
      "torch.Size([64])\n",
      "model.skip_layers.upsample.transp_conv.conv.weight\n",
      "torch.Size([96, 64, 2, 2, 2])\n",
      "model.skip_layers.upsample.transp_conv.conv.bias\n",
      "torch.Size([64])\n",
      "model.skip_layers.upsample.conv_block.conv1.conv.weight\n",
      "torch.Size([64, 128, 3, 3, 3])\n",
      "model.skip_layers.upsample.conv_block.conv2.conv.weight\n",
      "torch.Size([64, 64, 3, 3, 3])\n",
      "model.skip_layers.upsample.conv_block.norm1.weight\n",
      "torch.Size([64])\n",
      "model.skip_layers.upsample.conv_block.norm1.bias\n",
      "torch.Size([64])\n",
      "model.skip_layers.upsample.conv_block.norm2.weight\n",
      "torch.Size([64])\n",
      "model.skip_layers.upsample.conv_block.norm2.bias\n",
      "torch.Size([64])\n",
      "model.skip_layers.next_layer.downsample.conv1.conv.weight\n",
      "torch.Size([96, 64, 3, 3, 3])\n",
      "model.skip_layers.next_layer.downsample.conv2.conv.weight\n",
      "torch.Size([96, 96, 3, 3, 3])\n",
      "model.skip_layers.next_layer.downsample.norm1.weight\n",
      "torch.Size([96])\n",
      "model.skip_layers.next_layer.downsample.norm1.bias\n",
      "torch.Size([96])\n",
      "model.skip_layers.next_layer.downsample.norm2.weight\n",
      "torch.Size([96])\n",
      "model.skip_layers.next_layer.downsample.norm2.bias\n",
      "torch.Size([96])\n",
      "model.skip_layers.next_layer.upsample.transp_conv.conv.weight\n",
      "torch.Size([128, 96, 2, 2, 2])\n",
      "model.skip_layers.next_layer.upsample.transp_conv.conv.bias\n",
      "torch.Size([96])\n",
      "model.skip_layers.next_layer.upsample.conv_block.conv1.conv.weight\n",
      "torch.Size([96, 192, 3, 3, 3])\n",
      "model.skip_layers.next_layer.upsample.conv_block.conv2.conv.weight\n",
      "torch.Size([96, 96, 3, 3, 3])\n",
      "model.skip_layers.next_layer.upsample.conv_block.norm1.weight\n",
      "torch.Size([96])\n",
      "model.skip_layers.next_layer.upsample.conv_block.norm1.bias\n",
      "torch.Size([96])\n",
      "model.skip_layers.next_layer.upsample.conv_block.norm2.weight\n",
      "torch.Size([96])\n",
      "model.skip_layers.next_layer.upsample.conv_block.norm2.bias\n",
      "torch.Size([96])\n",
      "model.skip_layers.next_layer.next_layer.downsample.conv1.conv.weight\n",
      "torch.Size([128, 96, 3, 3, 3])\n",
      "model.skip_layers.next_layer.next_layer.downsample.conv2.conv.weight\n",
      "torch.Size([128, 128, 3, 3, 3])\n",
      "model.skip_layers.next_layer.next_layer.downsample.norm1.weight\n",
      "torch.Size([128])\n",
      "model.skip_layers.next_layer.next_layer.downsample.norm1.bias\n",
      "torch.Size([128])\n",
      "model.skip_layers.next_layer.next_layer.downsample.norm2.weight\n",
      "torch.Size([128])\n",
      "model.skip_layers.next_layer.next_layer.downsample.norm2.bias\n",
      "torch.Size([128])\n",
      "model.skip_layers.next_layer.next_layer.upsample.transp_conv.conv.weight\n",
      "torch.Size([192, 128, 2, 2, 2])\n",
      "model.skip_layers.next_layer.next_layer.upsample.transp_conv.conv.bias\n",
      "torch.Size([128])\n",
      "model.skip_layers.next_layer.next_layer.upsample.conv_block.conv1.conv.weight\n",
      "torch.Size([128, 256, 3, 3, 3])\n",
      "model.skip_layers.next_layer.next_layer.upsample.conv_block.conv2.conv.weight\n",
      "torch.Size([128, 128, 3, 3, 3])\n",
      "model.skip_layers.next_layer.next_layer.upsample.conv_block.norm1.weight\n",
      "torch.Size([128])\n",
      "model.skip_layers.next_layer.next_layer.upsample.conv_block.norm1.bias\n",
      "torch.Size([128])\n",
      "model.skip_layers.next_layer.next_layer.upsample.conv_block.norm2.weight\n",
      "torch.Size([128])\n",
      "model.skip_layers.next_layer.next_layer.upsample.conv_block.norm2.bias\n",
      "torch.Size([128])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.downsample.conv1.conv.weight\n",
      "torch.Size([192, 128, 3, 3, 3])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.downsample.conv2.conv.weight\n",
      "torch.Size([192, 192, 3, 3, 3])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.downsample.norm1.weight\n",
      "torch.Size([192])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.downsample.norm1.bias\n",
      "torch.Size([192])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.downsample.norm2.weight\n",
      "torch.Size([192])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.downsample.norm2.bias\n",
      "torch.Size([192])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.upsample.transp_conv.conv.weight\n",
      "torch.Size([256, 192, 2, 2, 2])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.upsample.transp_conv.conv.bias\n",
      "torch.Size([192])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.upsample.conv_block.conv1.conv.weight\n",
      "torch.Size([192, 384, 3, 3, 3])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.upsample.conv_block.conv2.conv.weight\n",
      "torch.Size([192, 192, 3, 3, 3])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.upsample.conv_block.norm1.weight\n",
      "torch.Size([192])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.upsample.conv_block.norm1.bias\n",
      "torch.Size([192])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.upsample.conv_block.norm2.weight\n",
      "torch.Size([192])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.upsample.conv_block.norm2.bias\n",
      "torch.Size([192])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.downsample.conv1.conv.weight\n",
      "torch.Size([256, 192, 3, 3, 3])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.downsample.conv2.conv.weight\n",
      "torch.Size([256, 256, 3, 3, 3])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.downsample.norm1.weight\n",
      "torch.Size([256])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.downsample.norm1.bias\n",
      "torch.Size([256])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.downsample.norm2.weight\n",
      "torch.Size([256])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.downsample.norm2.bias\n",
      "torch.Size([256])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.upsample.transp_conv.conv.weight\n",
      "torch.Size([384, 256, 2, 2, 2])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.upsample.transp_conv.conv.bias\n",
      "torch.Size([256])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.upsample.conv_block.conv1.conv.weight\n",
      "torch.Size([256, 512, 3, 3, 3])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.upsample.conv_block.conv2.conv.weight\n",
      "torch.Size([256, 256, 3, 3, 3])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.upsample.conv_block.norm1.weight\n",
      "torch.Size([256])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.upsample.conv_block.norm1.bias\n",
      "torch.Size([256])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.upsample.conv_block.norm2.weight\n",
      "torch.Size([256])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.upsample.conv_block.norm2.bias\n",
      "torch.Size([256])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.next_layer.downsample.conv1.conv.weight\n",
      "torch.Size([384, 256, 3, 3, 3])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.next_layer.downsample.conv2.conv.weight\n",
      "torch.Size([384, 384, 3, 3, 3])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.next_layer.downsample.norm1.weight\n",
      "torch.Size([384])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.next_layer.downsample.norm1.bias\n",
      "torch.Size([384])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.next_layer.downsample.norm2.weight\n",
      "torch.Size([384])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.next_layer.downsample.norm2.bias\n",
      "torch.Size([384])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.next_layer.upsample.transp_conv.conv.weight\n",
      "torch.Size([512, 384, 2, 2, 2])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.next_layer.upsample.transp_conv.conv.bias\n",
      "torch.Size([384])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.next_layer.upsample.conv_block.conv1.conv.weight\n",
      "torch.Size([384, 768, 3, 3, 3])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.next_layer.upsample.conv_block.conv2.conv.weight\n",
      "torch.Size([384, 384, 3, 3, 3])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.next_layer.upsample.conv_block.norm1.weight\n",
      "torch.Size([384])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.next_layer.upsample.conv_block.norm1.bias\n",
      "torch.Size([384])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.next_layer.upsample.conv_block.norm2.weight\n",
      "torch.Size([384])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.next_layer.upsample.conv_block.norm2.bias\n",
      "torch.Size([384])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.next_layer.next_layer.conv1.conv.weight\n",
      "torch.Size([512, 384, 3, 3, 3])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.next_layer.next_layer.conv2.conv.weight\n",
      "torch.Size([512, 512, 3, 3, 3])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.next_layer.next_layer.norm1.weight\n",
      "torch.Size([512])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.next_layer.next_layer.norm1.bias\n",
      "torch.Size([512])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.next_layer.next_layer.norm2.weight\n",
      "torch.Size([512])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.next_layer.next_layer.norm2.bias\n",
      "torch.Size([512])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.next_layer.super_head.conv.conv.weight\n",
      "torch.Size([3, 384, 1, 1, 1])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.next_layer.super_head.conv.conv.bias\n",
      "torch.Size([3])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.super_head.conv.conv.weight\n",
      "torch.Size([3, 256, 1, 1, 1])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.next_layer.super_head.conv.conv.bias\n",
      "torch.Size([3])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.super_head.conv.conv.weight\n",
      "torch.Size([3, 192, 1, 1, 1])\n",
      "model.skip_layers.next_layer.next_layer.next_layer.super_head.conv.conv.bias\n",
      "torch.Size([3])\n",
      "model.skip_layers.next_layer.next_layer.super_head.conv.conv.weight\n",
      "torch.Size([3, 128, 1, 1, 1])\n",
      "model.skip_layers.next_layer.next_layer.super_head.conv.conv.bias\n",
      "torch.Size([3])\n",
      "model.skip_layers.next_layer.super_head.conv.conv.weight\n",
      "torch.Size([3, 96, 1, 1, 1])\n",
      "model.skip_layers.next_layer.super_head.conv.conv.bias\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for ech in weights['state_dict']:\n",
    "#     if ech in ch_layers:\n",
    "    print(ech)    \n",
    "    print(weights['state_dict'][ech].shape)\n",
    "#         print(weights['state_dict'][ech].requires_grad)\n",
    "#         print(weights['state_dict'][ech].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ca71fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/polina/DeepLearningExamples/PyTorch/Segmentation/nnUNet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae1258fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnunet.nn_unet import NNUnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21028581",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = '/results/gbm_results/gbm_4a_resamp/fold-2/checkpoints/best_epoch=198-dice_mean=75.08.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df30b68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  model = NNUnet.load_from_checkpoint(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b971bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name in last_layers:\n",
    "       print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "50ad4011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NNUnet(\n",
       "  (model): DynUNet(\n",
       "    (input_block): UnetBasicBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(5, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    )\n",
       "    (downsamples): ModuleList(\n",
       "      (0): UnetBasicBlock(\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(64, 96, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (norm1): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (norm2): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      )\n",
       "      (1): UnetBasicBlock(\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(96, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (norm1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (norm2): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      )\n",
       "      (2): UnetBasicBlock(\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(128, 192, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (norm1): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (norm2): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      )\n",
       "      (3): UnetBasicBlock(\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(192, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (norm1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (norm2): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      )\n",
       "      (4): UnetBasicBlock(\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(256, 384, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (norm1): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (norm2): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck): UnetBasicBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(384, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    )\n",
       "    (upsamples): ModuleList(\n",
       "      (0): UnetUpBlock(\n",
       "        (transp_conv): Convolution(\n",
       "          (conv): ConvTranspose3d(512, 384, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "        )\n",
       "        (conv_block): UnetBasicBlock(\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv3d(768, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "          (norm1): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (norm2): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (1): UnetUpBlock(\n",
       "        (transp_conv): Convolution(\n",
       "          (conv): ConvTranspose3d(384, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "        )\n",
       "        (conv_block): UnetBasicBlock(\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "          (norm1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (norm2): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (2): UnetUpBlock(\n",
       "        (transp_conv): Convolution(\n",
       "          (conv): ConvTranspose3d(256, 192, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "        )\n",
       "        (conv_block): UnetBasicBlock(\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv3d(384, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "          (norm1): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (norm2): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (3): UnetUpBlock(\n",
       "        (transp_conv): Convolution(\n",
       "          (conv): ConvTranspose3d(192, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "        )\n",
       "        (conv_block): UnetBasicBlock(\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "          (norm1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (norm2): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (4): UnetUpBlock(\n",
       "        (transp_conv): Convolution(\n",
       "          (conv): ConvTranspose3d(128, 96, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "        )\n",
       "        (conv_block): UnetBasicBlock(\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv3d(192, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "          (norm1): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (norm2): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (5): UnetUpBlock(\n",
       "        (transp_conv): Convolution(\n",
       "          (conv): ConvTranspose3d(96, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "        )\n",
       "        (conv_block): UnetBasicBlock(\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "          (norm1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (norm2): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_block): UnetOutBlock(\n",
       "      (conv): Convolution(\n",
       "        (conv): Conv3d(64, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      )\n",
       "    )\n",
       "    (deep_supervision_heads): ModuleList(\n",
       "      (0): UnetOutBlock(\n",
       "        (conv): Convolution(\n",
       "          (conv): Conv3d(96, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): UnetOutBlock(\n",
       "        (conv): Convolution(\n",
       "          (conv): Conv3d(128, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "        )\n",
       "      )\n",
       "      (2): UnetOutBlock(\n",
       "        (conv): Convolution(\n",
       "          (conv): Conv3d(192, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "        )\n",
       "      )\n",
       "      (3): UnetOutBlock(\n",
       "        (conv): Convolution(\n",
       "          (conv): Conv3d(256, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "        )\n",
       "      )\n",
       "      (4): UnetOutBlock(\n",
       "        (conv): Convolution(\n",
       "          (conv): Conv3d(384, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (skip_layers): DynUNetSkipLayer(\n",
       "      (downsample): UnetBasicBlock(\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(5, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (norm1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (norm2): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      )\n",
       "      (upsample): UnetUpBlock(\n",
       "        (transp_conv): Convolution(\n",
       "          (conv): ConvTranspose3d(96, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "        )\n",
       "        (conv_block): UnetBasicBlock(\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "          (norm1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (norm2): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (next_layer): DynUNetSkipLayer(\n",
       "        (downsample): UnetBasicBlock(\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv3d(64, 96, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "          (norm1): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (norm2): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        )\n",
       "        (upsample): UnetUpBlock(\n",
       "          (transp_conv): Convolution(\n",
       "            (conv): ConvTranspose3d(128, 96, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "          )\n",
       "          (conv_block): UnetBasicBlock(\n",
       "            (conv1): Convolution(\n",
       "              (conv): Conv3d(192, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (conv2): Convolution(\n",
       "              (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "            (norm1): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (norm2): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          )\n",
       "        )\n",
       "        (next_layer): DynUNetSkipLayer(\n",
       "          (downsample): UnetBasicBlock(\n",
       "            (conv1): Convolution(\n",
       "              (conv): Conv3d(96, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (conv2): Convolution(\n",
       "              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "            (norm1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (norm2): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          )\n",
       "          (upsample): UnetUpBlock(\n",
       "            (transp_conv): Convolution(\n",
       "              (conv): ConvTranspose3d(192, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "            )\n",
       "            (conv_block): UnetBasicBlock(\n",
       "              (conv1): Convolution(\n",
       "                (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "              )\n",
       "              (conv2): Convolution(\n",
       "                (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "              )\n",
       "              (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "              (norm1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "              (norm2): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            )\n",
       "          )\n",
       "          (next_layer): DynUNetSkipLayer(\n",
       "            (downsample): UnetBasicBlock(\n",
       "              (conv1): Convolution(\n",
       "                (conv): Conv3d(128, 192, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "              )\n",
       "              (conv2): Convolution(\n",
       "                (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "              )\n",
       "              (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "              (norm1): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "              (norm2): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            )\n",
       "            (upsample): UnetUpBlock(\n",
       "              (transp_conv): Convolution(\n",
       "                (conv): ConvTranspose3d(256, 192, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "              )\n",
       "              (conv_block): UnetBasicBlock(\n",
       "                (conv1): Convolution(\n",
       "                  (conv): Conv3d(384, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "                )\n",
       "                (conv2): Convolution(\n",
       "                  (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "                )\n",
       "                (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "                (norm1): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "                (norm2): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "              )\n",
       "            )\n",
       "            (next_layer): DynUNetSkipLayer(\n",
       "              (downsample): UnetBasicBlock(\n",
       "                (conv1): Convolution(\n",
       "                  (conv): Conv3d(192, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "                )\n",
       "                (conv2): Convolution(\n",
       "                  (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "                )\n",
       "                (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "                (norm1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "                (norm2): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "              )\n",
       "              (upsample): UnetUpBlock(\n",
       "                (transp_conv): Convolution(\n",
       "                  (conv): ConvTranspose3d(384, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "                )\n",
       "                (conv_block): UnetBasicBlock(\n",
       "                  (conv1): Convolution(\n",
       "                    (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "                  )\n",
       "                  (conv2): Convolution(\n",
       "                    (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "                  )\n",
       "                  (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "                  (norm1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "                  (norm2): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "                )\n",
       "              )\n",
       "              (next_layer): DynUNetSkipLayer(\n",
       "                (downsample): UnetBasicBlock(\n",
       "                  (conv1): Convolution(\n",
       "                    (conv): Conv3d(256, 384, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "                  )\n",
       "                  (conv2): Convolution(\n",
       "                    (conv): Conv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "                  )\n",
       "                  (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "                  (norm1): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "                  (norm2): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "                )\n",
       "                (upsample): UnetUpBlock(\n",
       "                  (transp_conv): Convolution(\n",
       "                    (conv): ConvTranspose3d(512, 384, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "                  )\n",
       "                  (conv_block): UnetBasicBlock(\n",
       "                    (conv1): Convolution(\n",
       "                      (conv): Conv3d(768, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "                    )\n",
       "                    (conv2): Convolution(\n",
       "                      (conv): Conv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "                    )\n",
       "                    (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "                    (norm1): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "                    (norm2): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "                  )\n",
       "                )\n",
       "                (next_layer): UnetBasicBlock(\n",
       "                  (conv1): Convolution(\n",
       "                    (conv): Conv3d(384, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "                  )\n",
       "                  (conv2): Convolution(\n",
       "                    (conv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "                  )\n",
       "                  (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "                  (norm1): InstanceNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "                  (norm2): InstanceNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "                )\n",
       "                (super_head): UnetOutBlock(\n",
       "                  (conv): Convolution(\n",
       "                    (conv): Conv3d(384, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (super_head): UnetOutBlock(\n",
       "                (conv): Convolution(\n",
       "                  (conv): Conv3d(256, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (super_head): UnetOutBlock(\n",
       "              (conv): Convolution(\n",
       "                (conv): Conv3d(192, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (super_head): UnetOutBlock(\n",
       "            (conv): Convolution(\n",
       "              (conv): Conv3d(128, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (super_head): UnetOutBlock(\n",
       "          (conv): Convolution(\n",
       "            (conv): Conv3d(96, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (super_head): Identity()\n",
       "    )\n",
       "  )\n",
       "  (loss): LossBraTS(\n",
       "    (dice): DiceLoss()\n",
       "    (ce): BCEWithLogitsLoss()\n",
       "  )\n",
       "  (dice): Dice()\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25086efe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
